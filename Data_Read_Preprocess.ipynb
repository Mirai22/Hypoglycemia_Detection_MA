{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of the Dataset and Gaps Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for reading the xml files is influenced/copied by https://github.com/r-cui/GluPred/blob/master/preprocess/loader.py\n",
    "\n",
    "# this function extracts the glucose data from the xml files (features are the glucose value and the time)\n",
    "# then the data is rounded and resampled to 5 minutes\n",
    "# rounding can cause duplicates as to why only the first value is kept \n",
    "# resampling is needed to create a uniform time sequence and identify gaps\n",
    "def get_glc(root):\n",
    "    glucose = []\n",
    "    glucose_ts = []\n",
    "    for type_tag in root.findall('glucose_level/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        glucose.append(int(value))\n",
    "        glucose_ts.append(ts)\n",
    "        \n",
    "    glc_frame = [glucose_ts, glucose]\n",
    "    glc_frame = np.array(glc_frame)\n",
    "    df_glc = pd.DataFrame(glc_frame.T, columns=['ts', 'glucose'])\n",
    "    df_glc[\"ts\"] = df_glc[\"ts\"].dt.round('5min')\n",
    "    df_glc[\"ts\"] = df_glc[\"ts\"].drop_duplicates()\n",
    "\n",
    "    df_glc = df_glc.set_index('ts')\n",
    "    df_glc = df_glc.resample(\"5min\").asfreq()\n",
    "    df_glc = df_glc.reset_index()\n",
    "\n",
    "    return df_glc\n",
    "\n",
    "\n",
    "\n",
    "# this function extracts the basal insulin from the xml file which consists of the attributs: basal, temp_basal, and bolus\n",
    "# basal and temp_basal need to be combined and the original basal value is replaced wtih temp_basal\n",
    "# furthermore, basal is resampled to 5 minutes and the missing values are filled with the prior values\n",
    "# as basal is applied continously, it only changes when a new basal rate is set\n",
    "\n",
    "def get_basal(root):\n",
    "    basal = []\n",
    "    basal_ts = []\n",
    "\n",
    "    for type_tag in root.findall('basal/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        basal.append(float(value))\n",
    "        basal_ts.append(ts)\n",
    "\n",
    "    basal_frame = [basal_ts, basal]\n",
    "    basal_frame = np.array(basal_frame)\n",
    "    df_basal = pd.DataFrame(basal_frame.T, columns=['ts', 'basal'])\n",
    "\n",
    "    df_basal[\"ts\"] = pd.to_datetime(df_basal[\"ts\"])\n",
    "\n",
    "    df_basal = df_basal.set_index('ts')\n",
    "    df_basal = df_basal.resample(\"5min\").ffill()\n",
    "    df_basal = df_basal.reset_index()\n",
    "\n",
    "    return df_basal\n",
    "\n",
    "\n",
    "# temp_basal is a temporary dosage replacing the original basal rate, with a value of 0 the basal rate is suspended\n",
    "\n",
    "def get_temp_basal(root):\n",
    "    temp_basal = []\n",
    "    temp_basal_ts = []\n",
    "    temp_basal_dur = []\n",
    "\n",
    "    for type_tag in root.findall('temp_basal/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts_begin')\n",
    "        ts_end = type_tag.get('ts_end')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        ts_end = datetime.datetime.strptime(ts_end, \"%d-%m-%Y %H:%M:%S\")\n",
    "        temp_basal_dur.append(ts_end)\n",
    "        temp_basal.append(float(value))\n",
    "        temp_basal_ts.append(ts)\n",
    "\n",
    "    temp_basal_frame = [temp_basal_ts, temp_basal, temp_basal_dur]\n",
    "    temp_basal_frame = np.array(temp_basal_frame)\n",
    "    df_temp_basal = pd.DataFrame(temp_basal_frame.T, columns=['ts', 'temp_basal', 'basal_end'])\n",
    "\n",
    "    df_temp_basal[\"ts\"] = pd.to_datetime(df_temp_basal[\"ts\"] )\n",
    "    df_temp_basal[\"basal_end\"] = pd.to_datetime(df_temp_basal[\"basal_end\"])\n",
    "\n",
    "    # the data is rounded to 5 minutes\n",
    "    df_temp_basal[\"ts\"] = df_temp_basal[\"ts\"].dt.round('5min')\n",
    "    df_temp_basal[\"basal_end\"] = df_temp_basal[\"basal_end\"].dt.round('5min')\n",
    "    return df_temp_basal\n",
    "\n",
    "\n",
    "# This function aims to replace temp basal with the basal value, by identifying the start and end time of\n",
    "# temporal basal infusion\n",
    "\n",
    "def combine_basal_temp_basal(df_b, df_temp_b):\n",
    "\n",
    "    # first the basal dataframe and the dataframe with the temporal basal information are merged with a left join \n",
    "    # thus all time intervals in which no temporal basal was infused gets a nan value\n",
    "    # this enables to check for all instances without nan values to identify the basal dosage which needs to be replaced \n",
    "    combined_df = pd.merge(df_b, df_temp_b, on='ts', how='left')\n",
    "    combined_df[\"temp_basal\"] = combined_df[\"temp_basal\"].fillna(-1)\n",
    "\n",
    "    for i in range (0, len(combined_df)):\n",
    "        # condition to identify only the location of the basal rates which overlap with a temporal basal dosage\n",
    "        if((combined_df[\"temp_basal\"][i]  != -1)):\n",
    "            start_time = combined_df[\"ts\"][i]\n",
    "            end_time = combined_df[\"basal_end\"][i]\n",
    "            # after identifying the start and end, the values are replaced with the temporal dosage\n",
    "            combined_df.loc[(combined_df[\"ts\"] >= start_time) & (combined_df[\"ts\"] <= end_time), \"basal\"] = combined_df[\"temp_basal\"][i]  \n",
    "\n",
    "    combined_df = combined_df.drop(\"basal_end\", axis=1) \n",
    "    combined_df = combined_df.drop(\"temp_basal\", axis=1) \n",
    "    # the new dataframe is returned\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# this function extracts the data of bolus insulin from the xml file\n",
    "# bolus insulin is not infused continously, thus the start and endtime need to be identified to set the\n",
    "# correct bolus for each infusion duration in the later steps\n",
    "# the function returns a dataframe with the time, the bolus insulin dosage and the end time of infusion\n",
    "\n",
    "def get_bolus(root): \n",
    "    bolus = []\n",
    "    bolus_ts = []\n",
    "    bolus_end = []\n",
    "\n",
    "    for type_tag in root.findall('bolus/event'):\n",
    "        value = type_tag.get('dose')\n",
    "        ts = type_tag.get('ts_begin')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        ts_end = type_tag.get('ts_end')\n",
    "        ts_end = datetime.datetime.strptime(ts_end, \"%d-%m-%Y %H:%M:%S\")\n",
    "        bolus_ts.append(ts)\n",
    "        bolus_end.append(ts_end)\n",
    "        bolus.append(float(value))\n",
    "\n",
    "    bolus_frame = [bolus_ts, bolus, bolus_end]\n",
    "    bolus_frame = np.array(bolus_frame)\n",
    "    df_bolus =pd.DataFrame(bolus_frame.T, columns=['ts', 'bolus', 'bolus_end'])\n",
    "    \n",
    "    # the data is rounded to 5 minutes\n",
    "    df_bolus['ts'] = df_bolus['ts'].dt.round('5min')\n",
    "    df_bolus[\"ts\"] = df_bolus[\"ts\"].drop_duplicates()\n",
    "\n",
    "    # the data is rounded to 5 minutes\n",
    "    df_bolus['bolus_end'] = df_bolus['bolus_end'].dt.round('5min')\n",
    "    df_bolus['bolus_end'] = df_bolus['bolus_end'].drop_duplicates()\n",
    "\n",
    "    return df_bolus\n",
    "\n",
    "\n",
    "# this function extarcts the exercise data from the xml file for the 2018 cohort\n",
    "# The problem with the exercise data is that both patient cohorts use different wearables and parameters, hence to have \n",
    "# uniform data, the step count is converted to the magnitude of acceleration while the data in the 2020 cohort is kept as it is\n",
    "\n",
    "def get_step(root):\n",
    "    steps = []\n",
    "    steps_ts = []\n",
    "    for type_tag in root.findall('basis_steps/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        steps.append(int(value))\n",
    "        steps_ts.append(ts)\n",
    "\n",
    "    steps_frame = [steps_ts, steps]\n",
    "    steps_frame = np.array(steps_frame)\n",
    "    df_steps = pd.DataFrame(steps_frame.T, columns=['ts', 'steps'])\n",
    "\n",
    "    # the data is rounded to 5 minutes\n",
    "    df_steps['ts'] = df_steps['ts'].dt.round('5min')\n",
    "    df_steps['ts'] = df_steps['ts'].drop_duplicates()\n",
    "\n",
    "    # self reported sleep data is read to compensate for possible missing values during the night\n",
    "    df_sleep = get_sleep(root)\n",
    "    # step count values are replaced with 0 if the subject sleeps\n",
    "    combined_df = combine_step_sleep(df_steps, df_sleep)\n",
    "    # step count is converted to magnitude of acceleration\n",
    "    df_macc = convert_step_to_MOA(combined_df)\n",
    "\n",
    "    return df_macc\n",
    "\n",
    "\n",
    "# the step count could have missing values as to why the sleep time information is extracted to include 0 activity while sleeping\n",
    "# this functions extracts the reported time intervals of sleep and returns a dataframe with start and end time \n",
    "\n",
    "def get_sleep(root):\n",
    "    sleep_start = []\n",
    "    sleep_end = []\n",
    "    for type_tag in root.findall('sleep/event'):\n",
    "        start = type_tag.get('ts_end')\n",
    "        end = type_tag.get('ts_begin')\n",
    "        start = datetime.datetime.strptime(start, \"%d-%m-%Y %H:%M:%S\")\n",
    "        end = datetime.datetime.strptime(end, \"%d-%m-%Y %H:%M:%S\")\n",
    "        sleep_start.append(start)\n",
    "        sleep_end.append(end)\n",
    "        \n",
    "    sleep_frame = [sleep_start, sleep_end]\n",
    "    sleep_frame = np.array(sleep_frame)\n",
    "    df_sleep = pd.DataFrame(sleep_frame.T, columns=['ts', 'sleep_end'])\n",
    "\n",
    "    # the data is rounded to 5 minutes\n",
    "    df_sleep['ts'] = df_sleep['ts'].dt.round('5min')\n",
    "    df_sleep['sleep_end'] = df_sleep['sleep_end'].dt.round('5min')\n",
    "\n",
    "    return df_sleep\n",
    "\n",
    "\n",
    "# this function merges the step count with the reported sleep intervals and replaces the values with 0 if the person was sleeping \n",
    "\n",
    "def combine_step_sleep(df_step, df_sleep):\n",
    "\n",
    "    \n",
    "    combined_df = pd.merge(df_step, df_sleep, on='ts', how='left')\n",
    "    combined_df[\"steps\"] = combined_df[\"steps\"].fillna(-1)\n",
    "\n",
    "    for i in range (0, len(combined_df)):\n",
    "        if((combined_df[\"steps\"][i]  != -1)):\n",
    "            start_time = combined_df[\"ts\"][i]\n",
    "            end_time = combined_df[\"sleep_end\"][i]\n",
    "            # condition is checked and values are replaced\n",
    "            combined_df.loc[(combined_df[\"ts\"] >= start_time) & (combined_df[\"sleep_end\"] <= end_time), \"steps\"] = 0\n",
    "\n",
    "    combined_df = combined_df.drop(\"sleep_end\", axis=1) \n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "'''\n",
    "formulas are taken from:\n",
    "https://www.omnicalculator.com/physics/velocity : velocity\n",
    "https://www.omnicalculator.com/physics/magnitude-of-acceleration : acceleration and magnitude of acceleration\n",
    "\n",
    "- The formula for the magnitude of acceleration is the absolute value of the acceleration with a formula of |a| = sqrt(pow(x))\n",
    "- x in this case is the acceleration computed by the change in velocity divided by the time interval\n",
    "- so for converting step count into the magnitude of acceleration, the first step is to compute the velocity from the distance divided by the needed time\n",
    "- velocity = distance/time\n",
    "- acc = change in velocity/ change in time\n",
    "- moacc = |a| = sqrt(pow(x)) -> the absolute value was not taken since the 2020 also indicate negative acceleration\n",
    "\n",
    "1. First, the step count is converted to a distance of meters. Here, according to research the standard equality is 1 step = 0.74 - 0.76 meters. \n",
    "    -> velocity = (step count * 0.75) / (5)\n",
    "2. Secondly, the acceleration is calcualted from subtracting the considered velocity from the previous velocity (initial) and divide it again by (5)\n",
    "'''\n",
    "\n",
    "def convert_step_to_MOA(df_steps):\n",
    "    time_interval = 5  \n",
    "\n",
    "    df_steps['steps'] = df_steps['steps'].mul(0.75)\n",
    "    df_steps['velocity'] = df_steps['steps'].div(time_interval) \n",
    "    df_steps.loc[(df_steps[\"steps\"] == 0), \"velocity\"] = 0.0\n",
    "    df_steps['prior_velocity'] = df_steps['velocity'].shift(1, axis=0)\n",
    "    df_steps['difference'] = df_steps['velocity'] - df_steps['prior_velocity']\n",
    "    df_steps['acc'] = df_steps['difference'].div(time_interval) \n",
    "    df_steps['macc'] = df_steps['acc']\n",
    "\n",
    "    # here if the actual and prior velocity are 0 the value is set to 0 as well to not cause infinitiy values irritating the computation\n",
    "    df_steps.loc[(df_steps[\"velocity\"] == 0) & (df_steps[\"prior_velocity\"] == 0) , \"macc\"] = 0.0\n",
    "\n",
    "    df_macc = df_steps[['ts', 'macc']]\n",
    "\n",
    "    # scale the data to 0 and 1 \n",
    "    df_min_max_scaled = df_macc.copy() \n",
    "    df_min_max_scaled ['macc'] = (df_min_max_scaled['macc'] - df_min_max_scaled['macc'].min()) / (df_min_max_scaled['macc'].max() - df_min_max_scaled['macc'].min())  \n",
    "    \n",
    "    # the converted dataframe is returned\n",
    "    return df_min_max_scaled\n",
    "\n",
    "\n",
    "\n",
    "# this function extracts the acceleration data for the 2020 cohort from the xml file\n",
    "\n",
    "def get_macc(root):\n",
    "\n",
    "    macc = []\n",
    "    macc_ts = []\n",
    "\n",
    "    for type_tag in root.findall('acceleration/event'):\n",
    "        value = type_tag.get('value')\n",
    "        ts = type_tag.get('ts')\n",
    "        ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "        macc.append(float(value))\n",
    "        macc_ts.append(ts)\n",
    "    macc_frame = [macc_ts, macc]\n",
    "    macc_frame = np.array(macc_frame)\n",
    "    df_macc = pd.DataFrame(macc_frame.T, columns=['ts', 'macc'])\n",
    "\n",
    "    # it is resampled to 5 minutes and sum the magnitude of the single minutes, so that every 5 minutes the MoA summed over 5 minutes is presented\n",
    "    # since the acceleration is reported in one minute intervals\n",
    "    df_macc = df_macc.set_index('ts')\n",
    "    df_macc = df_macc.resample(\"5min\").sum()\n",
    "    df_macc = df_macc.reset_index()\n",
    "    # 0.0 possibly meant missing values \n",
    "    df_macc = df_macc.replace(0.0, np.nan)\n",
    "\n",
    "    # scale the data to 0 and 1 \n",
    "    df_min_max_scaled = df_macc.copy() \n",
    "    df_min_max_scaled ['macc'] = (df_min_max_scaled['macc'] - df_min_max_scaled['macc'].min()) / (df_min_max_scaled['macc'].max() - df_min_max_scaled['macc'].min())  \n",
    "    \n",
    "    return df_min_max_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function assigns the classes, it takes the start and end of the defined interval before the hypoglycemic event\n",
    "# furthermore, a list of all locations of hypoglycemic datapoints is given as input\n",
    "# only instances which were not assigned to another class are considered\n",
    "\n",
    "def Class_generation(df, start, end, class_number, list_hypo):\n",
    "    \n",
    "    # it is iterated over each hypoglycemic event and compute backwards with the given condition\n",
    "    for i in list_hypo:\n",
    "        current_time = pd.to_datetime(i)\n",
    "        start_time = current_time - datetime.timedelta(minutes = start)\n",
    "        end_time = current_time - datetime.timedelta(minutes = end)\n",
    "        # condition is checked, and the new class is assigned \n",
    "        df.loc[(df[\"ts\"] < start_time) & (df[\"ts\"] >= end_time) & (df[\"Class\"] == -1), \"Class\"] = class_number\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function identifies gaps and split the dataframe into multiple dataframes which do not contain any missing values\n",
    "# as input data the interpolated and extrapoalted dataframes, the subject_ID, and the version of the cohort are given\n",
    "# (with the help of chatpgt)\n",
    "def Remove_big_gaps(df, df2, subject_ID, version):\n",
    "\n",
    "    df_inter = df.copy().reset_index()\n",
    "    dataframes_inter = []\n",
    "\n",
    "    # the indexes of nan values are identified to split the original data based on those gaps\n",
    "    nan_mask_inter = df_inter['glucose'].isnull()\n",
    "    # consecutive nan values are identified \n",
    "    cumultative_sum_inter = nan_mask_inter.cumsum()\n",
    "    # groups of consecutive nan values and non nan values are build\n",
    "    groups_inter = df_inter.groupby(cumultative_sum_inter)\n",
    "\n",
    "    # it is iterated through the groups and only the dataframes are added to the list which do not contain nan values\n",
    "    for _, group in groups_inter: \n",
    "        if group['glucose'].isnull().all(): \n",
    "            continue\n",
    "        group = group.dropna()\n",
    "        dataframes_inter.append(group)\n",
    "\n",
    "    # each dataframe which does not contain any nan value is saved for the specific person\n",
    "    for i in range (0, len(dataframes_inter)):\n",
    "        file_name = \"GAPS_DATA/TRAIN/%s/%s_%i_%i_INTER.csv\" % (subject_ID,subject_ID, i, version)\n",
    "        dataframes_inter[i].to_csv(file_name)\n",
    "\n",
    "\n",
    "    # the same is also done for the extrapolated data\n",
    "        \n",
    "    df_extra = df2.copy().reset_index()\n",
    "    dataframes_extra = []\n",
    "\n",
    "    nan_mask_extra = df_extra['glucose'].isnull()\n",
    "    cumultative_sum_extra = nan_mask_extra.cumsum()\n",
    "    groups_extra = df_extra.groupby(cumultative_sum_extra)\n",
    "\n",
    "    for _, group in groups_extra: \n",
    "        if group['glucose'].isnull().all(): \n",
    "            continue\n",
    "        group = group.dropna()\n",
    "        dataframes_extra.append(group)\n",
    "\n",
    "    for i in range (0, len(dataframes_extra)):\n",
    "        file_name2 = \"GAPS_DATA/TEST/%s/%s_%i_%i_EXTRA.csv\" % (subject_ID,subject_ID, i, version)\n",
    "        dataframes_extra[i].to_csv(file_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the initial hypoglycemic events without any data imputation \n",
    "# it takes the train and test xml file of the subject as input as well as the subject id\n",
    "# it extracts the glucose data for the train and test files, then concatenate both files, and check the condition for \n",
    "# hypoglycemia to assign it the class 0\n",
    "# as output the number of hypoglycemic datapoints for the specific person is printed\n",
    "\n",
    "def Count_Initial_Hypo(TRAINFILE, TESTFILE, s_ID):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(0, len(TRAINFILE)):\n",
    "        root = ET.parse(TRAINFILE[i]).getroot()\n",
    "        root2 = ET.parse(TESTFILE[i]).getroot()\n",
    "\n",
    "        subject_ID = s_ID[count]\n",
    "        count = count +1\n",
    "\n",
    "        glucose = []\n",
    "        glucose_ts = []\n",
    "        for type_tag in root.findall('glucose_level/event'):\n",
    "            value = type_tag.get('value')\n",
    "            ts = type_tag.get('ts')\n",
    "            ts = datetime.datetime.strptime(ts, \"%d-%m-%Y %H:%M:%S\")\n",
    "            glucose.append(int(value))\n",
    "            glucose_ts.append(ts)\n",
    "            \n",
    "        glc_frame = [glucose_ts, glucose]\n",
    "        glc_frame = np.array(glc_frame)\n",
    "        df_glc = pd.DataFrame(glc_frame.T, columns=['ts', 'glucose'])\n",
    "\n",
    "\n",
    "        glucose2 = []\n",
    "        glucose_ts2 = []\n",
    "        for type_tag in root2.findall('glucose_level/event'):\n",
    "            value2 = type_tag.get('value')\n",
    "            ts2 = type_tag.get('ts')\n",
    "            ts2 = datetime.datetime.strptime(ts2, \"%d-%m-%Y %H:%M:%S\")\n",
    "            glucose2.append(int(value2))\n",
    "            glucose_ts2.append(ts2)\n",
    "            \n",
    "        glc_frame2 = [glucose_ts2, glucose2]\n",
    "        glc_frame2 = np.array(glc_frame2)\n",
    "        df_glc2 = pd.DataFrame(glc_frame2.T, columns=['ts', 'glucose'])\n",
    "\n",
    "        df_glc3 = pd.concat([df_glc, df_glc2])\n",
    "\n",
    "        df_glc3[\"Class\"] = 1\n",
    "        df_glc3.loc[df_glc3[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "        print(subject_ID)\n",
    "        print(np.bincount(df_glc3['Class']))\n",
    "        print(len(df_glc3['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function load the data, combines the single columns, fills in missing data and assings the classes\n",
    "# as input the file ordner which is either train or test, the subjects id, and finally the version which is 2018 or 2020 are given \n",
    "# linear interpolation and extrapolation are applied for missing values which are allowed to have a consecutive length of 2 hours\n",
    "\n",
    "def load_data(TRAINFILE, TESTFILE, s_ID, version):\n",
    "\n",
    "    count = 0\n",
    "    for i in range(0, len(TRAINFILE)):\n",
    "        root = ET.parse(TRAINFILE[i]).getroot()\n",
    "        root2 = ET.parse(TESTFILE[i]).getroot()\n",
    "\n",
    "        subject_ID = s_ID[count]\n",
    "        count = count +1\n",
    "        \n",
    "\n",
    "        # glucose, basal insulin, bolus insulin, and temp basal are stored as sepearte dataframes\n",
    "        df_glc = get_glc(root)\n",
    "        df_basal = get_basal(root)\n",
    "        df_bolus = get_bolus(root)\n",
    "        df_temp_basal = get_temp_basal(root)\n",
    "\n",
    "        # then the activity data is stored which calls either get_step() or get_macc() according to the chosen cohort\n",
    "        if version == 2018:\n",
    "            df_macc = get_step(root)\n",
    "        else:\n",
    "            df_macc = get_macc(root)\n",
    "\n",
    "        # the single dataframes are merged on the time and the subject id is added \n",
    "        df_list = [df_glc, df_basal, df_bolus, df_macc] \n",
    "        combined_df_train = df_list[0]\n",
    "        for i in range(1,len(df_list)):\n",
    "            combined_df_train = pd.merge(combined_df_train, df_list[i], on='ts', how='left')\n",
    "\n",
    "\n",
    "        # the same procedure is done for the test data\n",
    "        df_glc2 = get_glc(root2)\n",
    "        df_basal2 = get_basal(root2)\n",
    "        df_bolus2 = get_bolus(root2)\n",
    "        df_temp_basal2 = get_temp_basal(root2)\n",
    "\n",
    "        if version == 2018:\n",
    "            df_macc2 = get_step(root2)\n",
    "        else:\n",
    "            df_macc2 = get_macc(root2)\n",
    "\n",
    "        df_list2 = [df_glc2, df_basal2, df_bolus2, df_macc2] \n",
    "        combined_df_test = df_list2[0]\n",
    "        for i in range(1,len(df_list2)):\n",
    "            combined_df_test = pd.merge(combined_df_test, df_list2[i], on='ts', how='left')\n",
    "\n",
    "        # the train and test data are concatenated\n",
    "        combined_df = pd.concat([combined_df_train, combined_df_test])\n",
    "        combined_df[\"Subject_ID\"] = subject_ID\n",
    "        combined_df = combined_df.reset_index().drop(columns='index')\n",
    "\n",
    "        # the temporal basal replaces the original basal for the identified time intervalls of the train and then test files\n",
    "        combined_df = combine_basal_temp_basal(combined_df, df_temp_basal)\n",
    "        combined_df = combine_basal_temp_basal(combined_df, df_temp_basal2)\n",
    "\n",
    "        # the bolus insulin is integrated over the time interval on which it is applied and the the row bolus_end is deleted\n",
    "        for i in range (0, len(combined_df)):\n",
    "            if((combined_df[\"bolus\"][i]  != np.NaN)):\n",
    "                start_time = combined_df[\"ts\"][i]\n",
    "                end_time = combined_df[\"bolus_end\"][i]\n",
    "                combined_df.loc[(combined_df[\"ts\"] >= start_time) & (combined_df[\"ts\"] <= end_time), \"bolus\"] = combined_df[\"bolus\"][i]  \n",
    "        combined_df = combined_df.drop(\"bolus_end\", axis=1)\n",
    "\n",
    "        # the values are all converted to floats \n",
    "        combined_df['glucose'] = combined_df['glucose'].astype(str).astype(float)\n",
    "        combined_df['basal'] = combined_df['basal'].astype(str).astype(float)\n",
    "        combined_df['bolus'] = combined_df['bolus'].astype(str).astype(float)\n",
    "        combined_df['macc'] = combined_df['macc'].astype(str).astype(float)\n",
    "\n",
    "        # missing basal insulin is filled with the ffill and bfill method since the values are constantly infused\n",
    "        combined_df['basal'] = combined_df['basal'].fillna(method = 'ffill')\n",
    "        combined_df['basal'] = combined_df['basal'].fillna(method = 'bfill')\n",
    "        # missing bolus insulin is filled with 0 for nan values since most often missing values means that no bolus was infused\n",
    "        combined_df['bolus'] = combined_df['bolus'].fillna(0)\n",
    "\n",
    "\n",
    "        # the number of missing values for each parameter before data imputation is printed\n",
    "        print('Before Data Imputation')\n",
    "        print(subject_ID, 'intra:', combined_df.isna().sum())\n",
    "\n",
    "        # linear interpolation is applied for training data and linear extrapolation is applied for test data to fill some of the nan values in glucose and exercise data\n",
    "        combined_df2 = combined_df.copy()\n",
    "        # interpolation\n",
    "        combined_df = combined_df.interpolate(method = \"linear\", limit = 24, limit_direction=\"both\") \n",
    "        # extrapolation\n",
    "        combined_df2['glucose'] = combined_df2['glucose'].interpolate(method=\"slinear\", limit = 24, fill_value=\"extrapolate\", limit_direction=\"both\")\n",
    "        combined_df2['macc'] = combined_df2['macc'].interpolate(method=\"slinear\", limit = 24, fill_value=\"extrapolate\", limit_direction=\"both\")\n",
    "            \n",
    "\n",
    "        # remaining missing values in exercise data is filled with -1 indicating that no data was recorded\n",
    "        # those gaps were not removed, as glucose should be recorded continously to assign the classes, and they have the highest impact for the models\n",
    "        # and missing values could influence the performance significantly\n",
    "        # but it cannot be asserted that the patients will wear the wearable continously, as why the model should learn to ignore -1 values\n",
    "        combined_df['macc'] = combined_df['macc'].fillna(-1)\n",
    "        combined_df2['macc'] = combined_df2['macc'].fillna(-1)\n",
    "\n",
    "        # a column called Class is created and the value -1 is firstly assigned to each row to keep track of still available instances without a class\n",
    "        # then all glucose values below 70 mg/dL are given the Class 0\n",
    "        combined_df[\"Class\"] = -1\n",
    "        combined_df.loc[combined_df[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "\n",
    "        # a list is created containing the timestamps of hypoglycemic events \n",
    "        list_hypo = (combined_df.loc[combined_df[\"Class\"] == 0, \"ts\"]).to_numpy()\n",
    "\n",
    "        # the function Class_generation() is called with wanted intervalls before a hypoglycemic event in minutes\n",
    "        combined_df = Class_generation(combined_df, 0, 15, 1, list_hypo) # 0-15\n",
    "        combined_df = Class_generation(combined_df, 15, 30, 2, list_hypo)  # 15-30 \n",
    "        combined_df = Class_generation(combined_df, 30, 60, 3, list_hypo)  # 30-60\n",
    "        combined_df = Class_generation(combined_df, 60, 120, 4, list_hypo)  # 1-2 \n",
    "        combined_df = Class_generation(combined_df, 120, 240, 5, list_hypo) # 2-4\n",
    "        combined_df = Class_generation(combined_df, 240, 480, 6, list_hypo)  # 4-8\n",
    "        combined_df = Class_generation(combined_df, 480, 720, 7, list_hypo)  # 8-12\n",
    "        combined_df = Class_generation(combined_df, 720, 1440, 8, list_hypo)  # 12-24\n",
    "        combined_df = Class_generation(combined_df, 1440, 2880, 9, list_hypo)  # 24-48\n",
    "        # 10 could be no hypoglycemia \n",
    "        combined_df.loc[combined_df[\"Class\"] == -1, \"Class\"] = 10\n",
    "\n",
    "        # same procedure is done for the extrapolated data \n",
    "        combined_df2[\"Class\"] = -1\n",
    "        combined_df2.loc[combined_df2[\"glucose\"] <= 70, \"Class\"] = 0\n",
    "\n",
    "        list_hypo_2 = (combined_df2.loc[combined_df2[\"Class\"] == 0, \"ts\"]).to_numpy()\n",
    "\n",
    "        combined_df2 = Class_generation(combined_df2, 0, 15, 1, list_hypo_2)  # 0-15\n",
    "        combined_df2 = Class_generation(combined_df2, 15, 30, 2, list_hypo_2)  # 15-30 \n",
    "        combined_df2 = Class_generation(combined_df2, 30, 60, 3, list_hypo_2)  # 30-60\n",
    "        combined_df2 = Class_generation(combined_df2, 60, 120, 4, list_hypo_2)  # 1-2 \n",
    "        combined_df2 = Class_generation(combined_df2, 120, 240, 5, list_hypo_2) # 2-4\n",
    "        combined_df2 = Class_generation(combined_df2, 240, 480, 6, list_hypo_2)  # 4-8\n",
    "        combined_df2 = Class_generation(combined_df2, 480, 720, 7, list_hypo_2)  # 8-12\n",
    "        combined_df2 = Class_generation(combined_df2, 720, 1440, 8, list_hypo_2)  # 12-24\n",
    "        combined_df2 = Class_generation(combined_df2, 1440, 2880, 9, list_hypo_2)  # 24-48\n",
    "        combined_df2.loc[combined_df2[\"Class\"] == -1, \"Class\"] = 10\n",
    "\n",
    "\n",
    "        # the number of missing values for each parameter before data imputation is printed\n",
    "        print('After Linear')\n",
    "        print(subject_ID, 'intra:', combined_df.isna().sum())\n",
    "        print(subject_ID, 'extra:', combined_df2.isna().sum())\n",
    "\n",
    "        \n",
    "        # the distribution of the classes is printed for interpolated and extrapolated data, respectively\n",
    "        print(np.bincount(combined_df['Class']))\n",
    "        print(len(combined_df['Class']))\n",
    "\n",
    "        print(np.bincount(combined_df2['Class']))\n",
    "        print(len(combined_df2['Class']))\n",
    "\n",
    "        # the function Remove_big_gaps() is called to identify consecutive nan values \n",
    "        # and to create subdataframes for each patient without any gaps, which are then saved as single csv files\n",
    "        Remove_big_gaps(combined_df, combined_df2, subject_ID, version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function which contains the files with their corresponsing subject id, modus and version  \n",
    "# this function is highly influenced by the code of https://github.com/r-cui/GluPred/blob/master/preprocess/linker.py\n",
    "def main():\n",
    "    versions_arr = [2018, 2020]\n",
    "\n",
    "    for v in versions_arr:\n",
    "        # first the data of the 2018 is preprocess \n",
    "        if (v == 2018):\n",
    "            patient_index = [559, 563, 570, 575, 588, 591]\n",
    "            train_files = ['/OhioT1DM/2018/train/559-ws-training.xml', \n",
    "                        '/OhioT1DM/2018/train/563-ws-training.xml',\n",
    "                        '/OhioT1DM/2018/train/570-ws-training.xml',\n",
    "                        '/OhioT1DM/2018/train/575-ws-training.xml',\n",
    "                        '/OhioT1DM/2018/train/588-ws-training.xml',\n",
    "                        '/OhioT1DM/2018/train/591-ws-training.xml'\n",
    "                        ]\n",
    "\n",
    "\n",
    "            test_files = ['/OhioT1DM/2018/test/559-ws-testing.xml', \n",
    "                        '/OhioT1DM/2018/test/563-ws-testing.xml',\n",
    "                        '/OhioT1DM/2018/test/570-ws-testing.xml',\n",
    "                        '/OhioT1DM/2018/test/575-ws-testing.xml',\n",
    "                        '/OhioT1DM/2018/test/588-ws-testing.xml',\n",
    "                        '/OhioT1DM/2018/test/591-ws-testing.xml'\n",
    "                        ]\n",
    "        # second, the data of the 2020 is preprocess     \n",
    "        elif (v == 2020):\n",
    "            patient_index = [540, 544, 552, 567, 584, 596]\n",
    "            train_files = ['OhioT1DM/2020/train/540-ws-training.xml', \n",
    "                        '/OhioT1DM/2020/train/544-ws-training.xml',\n",
    "                        '/OhioT1DM/2020/train/552-ws-training.xml',\n",
    "                        '/OhioT1DM/2020/train/567-ws-training.xml',\n",
    "                        '/OhioT1DM/2020/train/584-ws-training.xml',\n",
    "                        '/OhioT1DM/2020/train/596-ws-training.xml'\n",
    "                        ]\n",
    "\n",
    "\n",
    "            test_files = ['/OhioT1DM/2020/test/540-ws-testing.xml', \n",
    "                        '/OhioT1DM/2020/test/544-ws-testing.xml',\n",
    "                        '/OhioT1DM/2020/test/552-ws-testing.xml',\n",
    "                        '/OhioT1DM/2020/test/567-ws-testing.xml',\n",
    "                        '/OhioT1DM/2020/test/584-ws-testing.xml',\n",
    "                        '/CODE/OhioT1DM/2020/test/596-ws-testing.xml'\n",
    "                        ]\n",
    "\n",
    "                \n",
    "        load_data(train_files, test_files, patient_index, version=v) \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
